# ml_model.py
import pandas as pd
import numpy as np
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation as LDA



from db_conn import fetch_data

# Define your SQL query to fetch the data
query = "SELECT [prompt],[response],[chat_precursor],[chat_id], CONVERT(varchar, [created_at], 120) as created_at FROM django_message"

# Fetch the data using the fetch_data function
data = fetch_data(query)

# print(data.head())  

# Load spacy model for lemmatization and stopword removal
nlp = spacy.load("en_core_web_sm")

# Load the Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

data['combined_text'] = data['prompt']

def preprocess_text(text):
    """
    This function preprocesses text by applying tokenization, stop word removal, and lemmatization using spaCy.
    """
    doc = nlp(text.lower())
    processed_text = " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])
    return processed_text



# Apply preprocessing to the dataset
data['processed_text'] = data['combined_text'].apply(preprocess_text)

data.dropna(subset=['processed_text'], inplace=True)

# Embedding Generation with Sentence-BERT
embeddings = model.encode(data['processed_text'].tolist(), convert_to_tensor=True)

# Clustering (KMeans)
num_clusters = 5  # Choose the number of clusters based on your analysis
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
data['cluster'] = kmeans.fit_predict(embeddings.cpu().detach().numpy())

vectorizer = CountVectorizer(max_features=1000)
tf_vectors = vectorizer.fit_transform(data['processed_text'])
lda = LDA(n_components=5, random_state=42)
lda_topics = lda.fit_transform(tf_vectors)

def get_top_keywords(lda_model, vectorizer, n_words=10):
    """
    Get the top n words for each topic generated by the LDA model.
    """
    keywords = []
    for topic_idx, topic in enumerate(lda_model.components_):
        top_keywords = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-n_words:]]
        keywords.append(" ".join(top_keywords))
    return keywords

# Get the keywords for each topic
topic_keywords = get_top_keywords(lda, vectorizer)

# Assigning topic keywords to each entry based on its dominant LDA topic
data['lda_topic'] = lda_topics.argmax(axis=1)
data['topic_keywords'] = data['lda_topic'].apply(lambda x: topic_keywords[x])

# Load Topics Table (assuming it has a column 'body' with predefined topics)
topics = pd.read_csv('C:/Users/abhor/OneDrive/Desktop/Project IQ/IQ_topics.csv')


# Create a function to match extracted keywords with predefined topics
def match_topic_keywords(keyword_list, topics_df):
    """
    Match extracted keywords with the predefined topics in the 'body' column and return the matched topics.
    """
    matched_labels = set()
    for keyword in keyword_list:
        matching_topics = topics_df[topics_df['body'].str.contains(keyword, case=False, na=False)]
        matched_labels.update(matching_topics['body'].tolist())
    return ", ".join(matched_labels) if matched_labels else "Unmatched"

# Apply the matching function to the extracted topic keywords
data['matched_topic'] = data['topic_keywords'].apply(lambda x: match_topic_keywords(x.split(), topics))

# Display results
# print(data[['combined_text', 'processed_text', 'cluster', 'lda_topic', 'topic_keywords', 'matched_topic']])

# Count the frequency of each matched topic
topic_counts = data['matched_topic'].value_counts().reset_index()
topic_counts.columns = ['topic', 'frequency']

# Display the top N topics by frequency (e.g., top 10)
top_topics = topic_counts.head(10)
# print(top_topics)

from textblob import TextBlob

# Function to compute sentiment
def analyze_sentiment(text):
    sentiment = TextBlob(text).sentiment.polarity
    if sentiment < 0:
        return 'negative'
    elif sentiment > 0:
        return 'positive'
    else:
        return 'neutral'

# Apply sentiment analysis to each message
data['sentiment'] = data['processed_text'].apply(analyze_sentiment)

# Filter for negative sentiment to identify concerns
concerns = data[data['sentiment'] == 'negative']

# Group concerns by topic and count occurrences
concern_counts = concerns['matched_topic'].value_counts().reset_index()
concern_counts.columns = ['topic', 'concern_count']

# Merge with topic frequency data to see both frequency and concern level
topic_summary = pd.merge(top_topics, concern_counts, on='topic', how='left').fillna(0)
topic_summary['concern_count'] = topic_summary['concern_count'].astype(int)

# Display topics with the highest number of concerns
# print(topic_summary.sort_values(by='concern_count', ascending=False))

# Get sample user concerns for each top topic
top_topic_concerns = {}

for topic in top_topics['topic']:
    # Filter for entries matching this topic with negative sentiment
    topic_concerns = concerns[concerns['matched_topic'] == topic]
    top_topic_concerns[topic] = topic_concerns['processed_text'].head(3).tolist()  # Top 3 concerns

# Display example concerns for each top topic
for topic, concern_texts in top_topic_concerns.items():
    print(f"Topic: {topic}")
    for text in concern_texts:
        print(f"- Concern: {text}")
    print("\n")

topics_concerns_data = []

# Loop through topics and concerns to populate the data list
for topic, concerns in top_topic_concerns.items():
    # Get concern count for this topic
    concern_count = int(topic_summary[topic_summary['topic'] == topic]['concern_count'].values[0])

    # Append each concern related to the topic to the data list
    for concern_text in concerns:
        topics_concerns_data.append({
            "topic": topic,
            "concern_text": concern_text,
            "concern_count": concern_count
        })

# Convert the list of dictionaries to a DataFrame
topics_concerns_df = pd.DataFrame(topics_concerns_data)

# Display the DataFrame to verify the contents
print(topics_concerns_df)